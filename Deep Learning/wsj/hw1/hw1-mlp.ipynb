{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_HIDDEN:  50\n",
      "LEARNING_RATE:  0.04\n",
      "BATCH_SIZE:  128\n",
      "NUM_EPOCH:  80\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Author: Shijie Wang\n",
    "ID: 2016010539\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "## Network architecture\n",
    "NUM_INPUT = 784  # Number of input neurons\n",
    "NUM_OUTPUT = 10  # Number of output neurons\n",
    "NUM_CHECK = 5  # Number of examples on which to check the gradient\n",
    "\n",
    "## Hyperparameters\n",
    "NUM_HIDDEN = 50\n",
    "LEARNING_RATE = 0.04\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCH = 80\n",
    "\n",
    "print(\"NUM_HIDDEN: \", NUM_HIDDEN)\n",
    "print(\"LEARNING_RATE: \", LEARNING_RATE)\n",
    "print(\"BATCH_SIZE: \", BATCH_SIZE)\n",
    "print(\"NUM_EPOCH: \", NUM_EPOCH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a vector w containing all the weights and biased vectors, extract\n",
    "# and return the individual weights and biases W1, b1, W2, b2.\n",
    "def unpack (w):\n",
    "    W1 = np.reshape(w[:NUM_INPUT * NUM_HIDDEN],(NUM_INPUT,NUM_HIDDEN))\n",
    "    w = w[NUM_INPUT * NUM_HIDDEN:]\n",
    "    b1 = np.reshape(w[:NUM_HIDDEN], NUM_HIDDEN)\n",
    "    w = w[NUM_HIDDEN:]\n",
    "    W2 = np.reshape(w[:NUM_HIDDEN*NUM_OUTPUT], (NUM_HIDDEN,NUM_OUTPUT))\n",
    "    w = w[NUM_HIDDEN*NUM_OUTPUT:]\n",
    "    b2 = np.reshape(w,NUM_OUTPUT)\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# Given individual weights and biases W1, b1, W2, b2, concatenate them and\n",
    "# return a vector w containing all of them.\n",
    "def pack (W1, b1, W2, b2):\n",
    "    W1_ = np.reshape(W1,NUM_INPUT*NUM_HIDDEN)\n",
    "    # print(W1_.shape)\n",
    "    W2_ = np.reshape(W2,NUM_HIDDEN*NUM_OUTPUT)\n",
    "    # print(W2_.shape)\n",
    "    w = np.concatenate((W1_,b1, W2_, b2))\n",
    "    # print(w.shape)\n",
    "    return w\n",
    "\n",
    "# Load the images and labels from a specified dataset (train or test).\n",
    "def loadData (which):\n",
    "    images = np.load(\"./data/mnist_{}_images.npy\".format(which))\n",
    "    labels = np.load(\"./data/mnist_{}_labels.npy\".format(which))\n",
    "    return images, labels\n",
    "\n",
    "def ReLU(z):\n",
    "    mask = z>0\n",
    "    a = z * mask\n",
    "    return a\n",
    "\n",
    "def Softmax(z):\n",
    "    e = np.exp(z)\n",
    "    s = np.sum(e,1).reshape(-1,1)\n",
    "    return e/s\n",
    "\n",
    "def CELoss(y_out,y):\n",
    "    return -(np.sum(y*np.log(y_out)))/BATCH_SIZE\n",
    "\n",
    "def sgn(x):\n",
    "    return (x>0)+0\n",
    "\n",
    "def shuffleDataset(x,y):\n",
    "    permutation = np.random.permutation(y.shape[0])\n",
    "    shuffled_x = x[permutation, :]\n",
    "    shuffled_y = y[permutation, :]\n",
    "    return shuffled_x, shuffled_y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Forward Propagation\n",
    "# Given training images X, associated labels Y, and a vector of combined weights\n",
    "# and bias terms w, compute and return the cross-entropy (CE) loss.\n",
    "\n",
    "def fCE (X, Y, w):\n",
    "    # print(X.shape)\n",
    "    ## your code here\n",
    "    W1, b1, W2, b2 = unpack(w)\n",
    "    loss = 0.0\n",
    "    ## W1(784,50) b1(50) W2(50,10) b2(10)\n",
    "    ## X(N,784) Y(N,10)\n",
    "    z1 = np.matmul(X,W1) + b1   #z1(N,50)\n",
    "    h1 = ReLU(z1)               #h1(N,50)\n",
    "    z2 = np.matmul(h1,W2) + b2  #z2(N,10)\n",
    "    a2 = Softmax(z2)\n",
    "    loss = CELoss(a2,Y)\n",
    "    y_pred = np.argmax(a2,1).reshape(-1)\n",
    "    y_act = np.argmax(Y,1).reshape(-1)\n",
    "    acc = np.sum(y_pred==y_act)/len(y_pred)\n",
    "    return loss,z1,h1,z2,a2,acc\n",
    "\n",
    "## 2. Backward Propagation\n",
    "# Given training images X, associated labels Y, and a vector of combined weights\n",
    "# and bias terms w, compute and return the gradient of fCE. \n",
    "def gradCE (X, Y, w, z1, h1, z2, a2):\n",
    "    W1, b1, W2, b2 = unpack(w)\n",
    "    ## your code here\n",
    "    delta_W_2 = (np.matmul(h1.T,(a2-Y)))/BATCH_SIZE     #(50,10)\n",
    "    delta_b_2 = np.mean(a2-Y,0)                         #(10)\n",
    "    delta_W_1 = (np.matmul(X.T,np.matmul((a2-Y),W2.T)*sgn(z1)))/BATCH_SIZE      #(784,50)\n",
    "    delta_b_1 = np.mean((np.matmul((a2-Y),W2.T)*sgn(z1)),0)                     #(50)\n",
    "    delta = pack(delta_W_1, delta_b_1, delta_W_2, delta_b_2)\n",
    "    return delta\n",
    "\n",
    "## 3. Parameter Update\n",
    "# Given training and testing datasets and an initial set of weights/biases,\n",
    "# train the NN.\n",
    "def train(trainX, trainY, testX, testY, w):\n",
    "    ## your code here\n",
    "    for epoch in range(NUM_EPOCH):\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        shuffled_trainX, shuffled_trainY = shuffleDataset(trainX, trainY)\n",
    "        splits_x = np.array_split(shuffled_trainX, shuffled_trainX.shape[0] / BATCH_SIZE)\n",
    "        splits_y = np.array_split(shuffled_trainY, shuffled_trainY.shape[0] / BATCH_SIZE)\n",
    "        for x, y in zip(splits_x, splits_y):\n",
    "            train_loss,z1,h1,z2,a2,train_acc = fCE(x, y, w)\n",
    "            delta = gradCE(x, y, w, z1, h1, z2, a2)\n",
    "            w -= LEARNING_RATE*delta\n",
    "        print(\"Epoch {}: \".format(epoch + 1))\n",
    "        print(\"train loss: {:.3f}, train accuracy: {:.3f}\".format(train_loss, train_acc))\n",
    "        test_loss, _, _, _, _, test_acc = fCE(testX, testY, w)\n",
    "        print(\"test loss: {:.3f}, test accuracy: {:.3f}\\n\".format(test_loss, test_acc))\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape(trainX):  (10000, 784)\n",
      "shape(testX):  (5000, 784)\n",
      "shape(trainY):  (10000, 10)\n",
      "shape(testY):  (5000, 10)\n",
      "Shape of w: (39760,)\n",
      "Epoch 1: \n",
      "train loss: 1.417, train accuracy: 0.797\n",
      "test loss: 57.339, test accuracy: 0.751\n",
      "\n",
      "Epoch 2: \n",
      "train loss: 0.865, train accuracy: 0.805\n",
      "test loss: 32.869, test accuracy: 0.817\n",
      "\n",
      "Epoch 3: \n",
      "train loss: 0.619, train accuracy: 0.859\n",
      "test loss: 24.437, test accuracy: 0.846\n",
      "\n",
      "Epoch 4: \n",
      "train loss: 0.513, train accuracy: 0.875\n",
      "test loss: 20.502, test accuracy: 0.866\n",
      "\n",
      "Epoch 5: \n",
      "train loss: 0.461, train accuracy: 0.875\n",
      "test loss: 18.326, test accuracy: 0.874\n",
      "\n",
      "Epoch 6: \n",
      "train loss: 0.409, train accuracy: 0.875\n",
      "test loss: 16.958, test accuracy: 0.884\n",
      "\n",
      "Epoch 7: \n",
      "train loss: 0.305, train accuracy: 0.906\n",
      "test loss: 15.883, test accuracy: 0.890\n",
      "\n",
      "Epoch 8: \n",
      "train loss: 0.378, train accuracy: 0.891\n",
      "test loss: 15.190, test accuracy: 0.893\n",
      "\n",
      "Epoch 9: \n",
      "train loss: 0.295, train accuracy: 0.906\n",
      "test loss: 14.671, test accuracy: 0.896\n",
      "\n",
      "Epoch 10: \n",
      "train loss: 0.320, train accuracy: 0.898\n",
      "test loss: 14.272, test accuracy: 0.898\n",
      "\n",
      "Epoch 11: \n",
      "train loss: 0.234, train accuracy: 0.945\n",
      "test loss: 13.810, test accuracy: 0.903\n",
      "\n",
      "Epoch 12: \n",
      "train loss: 0.325, train accuracy: 0.891\n",
      "test loss: 13.525, test accuracy: 0.905\n",
      "\n",
      "Epoch 13: \n",
      "train loss: 0.275, train accuracy: 0.930\n",
      "test loss: 13.371, test accuracy: 0.904\n",
      "\n",
      "Epoch 14: \n",
      "train loss: 0.249, train accuracy: 0.930\n",
      "test loss: 13.048, test accuracy: 0.907\n",
      "\n",
      "Epoch 15: \n",
      "train loss: 0.241, train accuracy: 0.914\n",
      "test loss: 12.929, test accuracy: 0.907\n",
      "\n",
      "Epoch 16: \n",
      "train loss: 0.214, train accuracy: 0.953\n",
      "test loss: 12.745, test accuracy: 0.908\n",
      "\n",
      "Epoch 17: \n",
      "train loss: 0.127, train accuracy: 0.977\n",
      "test loss: 12.565, test accuracy: 0.909\n",
      "\n",
      "Epoch 18: \n",
      "train loss: 0.299, train accuracy: 0.891\n",
      "test loss: 12.352, test accuracy: 0.910\n",
      "\n",
      "Epoch 19: \n",
      "train loss: 0.240, train accuracy: 0.930\n",
      "test loss: 12.284, test accuracy: 0.911\n",
      "\n",
      "Epoch 20: \n",
      "train loss: 0.258, train accuracy: 0.930\n",
      "test loss: 12.147, test accuracy: 0.912\n",
      "\n",
      "Epoch 21: \n",
      "train loss: 0.241, train accuracy: 0.930\n",
      "test loss: 12.068, test accuracy: 0.912\n",
      "\n",
      "Epoch 22: \n",
      "train loss: 0.312, train accuracy: 0.930\n",
      "test loss: 11.940, test accuracy: 0.912\n",
      "\n",
      "Epoch 23: \n",
      "train loss: 0.226, train accuracy: 0.938\n",
      "test loss: 11.905, test accuracy: 0.915\n",
      "\n",
      "Epoch 24: \n",
      "train loss: 0.353, train accuracy: 0.914\n",
      "test loss: 11.824, test accuracy: 0.916\n",
      "\n",
      "Epoch 25: \n",
      "train loss: 0.175, train accuracy: 0.953\n",
      "test loss: 11.701, test accuracy: 0.915\n",
      "\n",
      "Epoch 26: \n",
      "train loss: 0.266, train accuracy: 0.906\n",
      "test loss: 11.563, test accuracy: 0.918\n",
      "\n",
      "Epoch 27: \n",
      "train loss: 0.250, train accuracy: 0.930\n",
      "test loss: 11.450, test accuracy: 0.917\n",
      "\n",
      "Epoch 28: \n",
      "train loss: 0.195, train accuracy: 0.953\n",
      "test loss: 11.438, test accuracy: 0.917\n",
      "\n",
      "Epoch 29: \n",
      "train loss: 0.308, train accuracy: 0.906\n",
      "test loss: 11.360, test accuracy: 0.920\n",
      "\n",
      "Epoch 30: \n",
      "train loss: 0.182, train accuracy: 0.961\n",
      "test loss: 11.228, test accuracy: 0.919\n",
      "\n",
      "Epoch 31: \n",
      "train loss: 0.255, train accuracy: 0.953\n",
      "test loss: 11.258, test accuracy: 0.919\n",
      "\n",
      "Epoch 32: \n",
      "train loss: 0.099, train accuracy: 0.977\n",
      "test loss: 11.072, test accuracy: 0.922\n",
      "\n",
      "Epoch 33: \n",
      "train loss: 0.156, train accuracy: 0.977\n",
      "test loss: 11.207, test accuracy: 0.919\n",
      "\n",
      "Epoch 34: \n",
      "train loss: 0.264, train accuracy: 0.914\n",
      "test loss: 10.978, test accuracy: 0.922\n",
      "\n",
      "Epoch 35: \n",
      "train loss: 0.150, train accuracy: 0.953\n",
      "test loss: 10.972, test accuracy: 0.921\n",
      "\n",
      "Epoch 36: \n",
      "train loss: 0.163, train accuracy: 0.961\n",
      "test loss: 10.861, test accuracy: 0.921\n",
      "\n",
      "Epoch 37: \n",
      "train loss: 0.173, train accuracy: 0.953\n",
      "test loss: 10.762, test accuracy: 0.923\n",
      "\n",
      "Epoch 38: \n",
      "train loss: 0.252, train accuracy: 0.953\n",
      "test loss: 10.766, test accuracy: 0.924\n",
      "\n",
      "Epoch 39: \n",
      "train loss: 0.185, train accuracy: 0.938\n",
      "test loss: 10.634, test accuracy: 0.924\n",
      "\n",
      "Epoch 40: \n",
      "train loss: 0.197, train accuracy: 0.953\n",
      "test loss: 10.710, test accuracy: 0.923\n",
      "\n",
      "Epoch 41: \n",
      "train loss: 0.176, train accuracy: 0.945\n",
      "test loss: 10.541, test accuracy: 0.924\n",
      "\n",
      "Epoch 42: \n",
      "train loss: 0.250, train accuracy: 0.891\n",
      "test loss: 10.539, test accuracy: 0.925\n",
      "\n",
      "Epoch 43: \n",
      "train loss: 0.182, train accuracy: 0.961\n",
      "test loss: 10.463, test accuracy: 0.926\n",
      "\n",
      "Epoch 44: \n",
      "train loss: 0.105, train accuracy: 0.969\n",
      "test loss: 10.421, test accuracy: 0.926\n",
      "\n",
      "Epoch 45: \n",
      "train loss: 0.153, train accuracy: 0.953\n",
      "test loss: 10.414, test accuracy: 0.928\n",
      "\n",
      "Epoch 46: \n",
      "train loss: 0.147, train accuracy: 0.953\n",
      "test loss: 10.302, test accuracy: 0.928\n",
      "\n",
      "Epoch 47: \n",
      "train loss: 0.132, train accuracy: 0.945\n",
      "test loss: 10.289, test accuracy: 0.928\n",
      "\n",
      "Epoch 48: \n",
      "train loss: 0.154, train accuracy: 0.969\n",
      "test loss: 10.298, test accuracy: 0.928\n",
      "\n",
      "Epoch 49: \n",
      "train loss: 0.133, train accuracy: 0.945\n",
      "test loss: 10.189, test accuracy: 0.929\n",
      "\n",
      "Epoch 50: \n",
      "train loss: 0.160, train accuracy: 0.961\n",
      "test loss: 10.143, test accuracy: 0.928\n",
      "\n",
      "Epoch 51: \n",
      "train loss: 0.092, train accuracy: 0.969\n",
      "test loss: 10.036, test accuracy: 0.929\n",
      "\n",
      "Epoch 52: \n",
      "train loss: 0.182, train accuracy: 0.938\n",
      "test loss: 10.023, test accuracy: 0.929\n",
      "\n",
      "Epoch 53: \n",
      "train loss: 0.135, train accuracy: 0.961\n",
      "test loss: 10.172, test accuracy: 0.930\n",
      "\n",
      "Epoch 54: \n",
      "train loss: 0.141, train accuracy: 0.984\n",
      "test loss: 9.972, test accuracy: 0.929\n",
      "\n",
      "Epoch 55: \n",
      "train loss: 0.153, train accuracy: 0.977\n",
      "test loss: 9.902, test accuracy: 0.931\n",
      "\n",
      "Epoch 56: \n",
      "train loss: 0.127, train accuracy: 0.969\n",
      "test loss: 9.845, test accuracy: 0.929\n",
      "\n",
      "Epoch 57: \n",
      "train loss: 0.167, train accuracy: 0.953\n",
      "test loss: 9.967, test accuracy: 0.929\n",
      "\n",
      "Epoch 58: \n",
      "train loss: 0.197, train accuracy: 0.945\n",
      "test loss: 9.811, test accuracy: 0.930\n",
      "\n",
      "Epoch 59: \n",
      "train loss: 0.114, train accuracy: 0.961\n",
      "test loss: 9.751, test accuracy: 0.930\n",
      "\n",
      "Epoch 60: \n",
      "train loss: 0.183, train accuracy: 0.953\n",
      "test loss: 9.660, test accuracy: 0.932\n",
      "\n",
      "Epoch 61: \n",
      "train loss: 0.207, train accuracy: 0.938\n",
      "test loss: 9.677, test accuracy: 0.930\n",
      "\n",
      "Epoch 62: \n",
      "train loss: 0.072, train accuracy: 0.984\n",
      "test loss: 9.568, test accuracy: 0.932\n",
      "\n",
      "Epoch 63: \n",
      "train loss: 0.093, train accuracy: 0.992\n",
      "test loss: 9.633, test accuracy: 0.931\n",
      "\n",
      "Epoch 64: \n",
      "train loss: 0.093, train accuracy: 0.969\n",
      "test loss: 9.572, test accuracy: 0.933\n",
      "\n",
      "Epoch 65: \n",
      "train loss: 0.095, train accuracy: 0.969\n",
      "test loss: 9.549, test accuracy: 0.933\n",
      "\n",
      "Epoch 66: \n",
      "train loss: 0.174, train accuracy: 0.938\n",
      "test loss: 9.528, test accuracy: 0.932\n",
      "\n",
      "Epoch 67: \n",
      "train loss: 0.126, train accuracy: 0.984\n",
      "test loss: 9.428, test accuracy: 0.932\n",
      "\n",
      "Epoch 68: \n",
      "train loss: 0.150, train accuracy: 0.938\n",
      "test loss: 9.417, test accuracy: 0.932\n",
      "\n",
      "Epoch 69: \n",
      "train loss: 0.123, train accuracy: 0.984\n",
      "test loss: 9.349, test accuracy: 0.933\n",
      "\n",
      "Epoch 70: \n",
      "train loss: 0.071, train accuracy: 0.984\n",
      "test loss: 9.329, test accuracy: 0.934\n",
      "\n",
      "Epoch 71: \n",
      "train loss: 0.053, train accuracy: 0.992\n",
      "test loss: 9.575, test accuracy: 0.934\n",
      "\n",
      "Epoch 72: \n",
      "train loss: 0.108, train accuracy: 0.961\n",
      "test loss: 9.337, test accuracy: 0.934\n",
      "\n",
      "Epoch 73: \n",
      "train loss: 0.115, train accuracy: 0.953\n",
      "test loss: 9.253, test accuracy: 0.934\n",
      "\n",
      "Epoch 74: \n",
      "train loss: 0.147, train accuracy: 0.984\n",
      "test loss: 9.265, test accuracy: 0.936\n",
      "\n",
      "Epoch 75: \n",
      "train loss: 0.132, train accuracy: 0.953\n",
      "test loss: 9.191, test accuracy: 0.934\n",
      "\n",
      "Epoch 76: \n",
      "train loss: 0.115, train accuracy: 0.969\n",
      "test loss: 9.198, test accuracy: 0.934\n",
      "\n",
      "Epoch 77: \n",
      "train loss: 0.080, train accuracy: 0.961\n",
      "test loss: 9.198, test accuracy: 0.934\n",
      "\n",
      "Epoch 78: \n",
      "train loss: 0.090, train accuracy: 0.984\n",
      "test loss: 9.114, test accuracy: 0.936\n",
      "\n",
      "Epoch 79: \n",
      "train loss: 0.066, train accuracy: 0.992\n",
      "test loss: 9.138, test accuracy: 0.935\n",
      "\n",
      "Epoch 80: \n",
      "train loss: 0.076, train accuracy: 0.984\n",
      "test loss: 9.100, test accuracy: 0.935\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    start_time = time.time()\n",
    "    trainX, trainY = loadData(\"train\")\n",
    "    testX, testY = loadData(\"test\")\n",
    "\n",
    "    print(\"shape(trainX): \", trainX.shape)\n",
    "    print(\"shape(testX): \", testX.shape)\n",
    "    print(\"shape(trainY): \", trainY.shape)\n",
    "    print(\"shape(testY): \", testY.shape)\n",
    "\n",
    "    # Initialize weights randomly\n",
    "    W1 = 2*(np.random.random(size=(NUM_INPUT, NUM_HIDDEN))/NUM_INPUT**0.5) - 1./NUM_INPUT**0.5\n",
    "    b1 = 0.01 * np.ones(NUM_HIDDEN)\n",
    "    W2 = 2*(np.random.random(size=(NUM_HIDDEN, NUM_OUTPUT))/NUM_HIDDEN**0.5) - 1./NUM_HIDDEN**0.5\n",
    "    b2 = 0.01 * np.ones(NUM_OUTPUT)\n",
    "\n",
    "    w = pack(W1, b1, W2, b2)\n",
    "    print(\"Shape of w:\",w.shape)\n",
    "    \n",
    "    # # Train the network and report the accuracy on the training and test set.\n",
    "    train(trainX, trainY, testX, testY, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is the hyperparameters I choose:\n",
    "\n",
    "NUM_HIDDEN = 50\n",
    "\n",
    "LEARNING_RATE = 0.04\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "NUM_EPOCH = 80\n",
    "\n",
    "\n",
    "#### In epoch 80, I get:\n",
    "\n",
    "train loss: 0.081, train accuracy: 0.977\n",
    "\n",
    "test loss: 8.617, test accuracy: 0.937"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}